"""Alignment utilities for transcription results."""

from dataclasses import dataclass


@dataclass
class AlignedToken:
    """A token with timing information."""
    id: int
    text: str
    start: float
    duration: float
    end: float = 0.0

    def __post_init__(self) -> None:
        self.end = self.start + self.duration


@dataclass
class AlignedSentence:
    """A sentence composed of aligned tokens."""
    text: str
    tokens: list[AlignedToken]
    start: float = 0.0
    end: float = 0.0
    duration: float = 0.0

    def __post_init__(self) -> None:
        self.tokens = list(sorted(self.tokens, key=lambda x: x.start))
        self.start = self.tokens[0].start
        self.end = self.tokens[-1].end
        self.duration = self.end - self.start


@dataclass
class TranscriptionResult:
    """Result of a transcription with timing information."""
    text: str
    sentences: list[AlignedSentence]

    def __post_init__(self) -> None:
        self.text = self.text.strip()

    @property
    def tokens(self) -> list[AlignedToken]:
        return [token for sentence in self.sentences for token in sentence.tokens]


def tokens_to_sentences(tokens: list[AlignedToken]) -> list[AlignedSentence]:
    """Group tokens into sentences based on punctuation."""
    sentences = []
    current_tokens = []

    for idx, token in enumerate(tokens):
        current_tokens.append(token)

        if (
            "!" in token.text
            or "?" in token.text
            or "。" in token.text
            or "？" in token.text
            or "！" in token.text
            or (
                "." in token.text
                and (idx == len(tokens) - 1 or " " in tokens[idx + 1].text)
            )
        ):
            sentence_text = "".join(t.text for t in current_tokens)
            sentence = AlignedSentence(text=sentence_text, tokens=current_tokens)
            sentences.append(sentence)
            current_tokens = []

    if current_tokens:
        sentence_text = "".join(t.text for t in current_tokens)
        sentence = AlignedSentence(text=sentence_text, tokens=current_tokens)
        sentences.append(sentence)

    return sentences


def sentences_to_result(sentences: list[AlignedSentence]) -> TranscriptionResult:
    """Create a TranscriptionResult from sentences."""
    return TranscriptionResult("".join(sentence.text for sentence in sentences), sentences)


def _has_uniform_timestamps(tokens: list[AlignedToken]) -> bool:
    """Check if all tokens have the same timestamp (no per-token timing)."""
    if len(tokens) < 2:
        return True
    first_start = tokens[0].start
    first_end = tokens[0].end
    return all(t.start == first_start and t.end == first_end for t in tokens)


def _merge_text_based(
    a: list[AlignedToken],
    b: list[AlignedToken],
    min_match_len: int = 10,
) -> list[AlignedToken]:
    """Merge using text-based matching when timestamps aren't reliable.
    
    This finds contiguous sequences of matching token IDs between the
    end of chunk A and the start of chunk B. Requires a minimum match
    length to avoid false positives from common tokens.
    """
    if not a or not b:
        return b if not a else a
    
    # Only search in expected overlap regions:
    # - Last third of A (where overlap should be)
    # - First third of B (where overlap should be)
    # This avoids matching unrelated parts of the chunks
    search_len_a = min(len(a) // 3 + 10, len(a))
    search_len_b = min(len(b) // 3 + 10, len(b))
    
    overlap_a = a[-search_len_a:]
    overlap_b = b[:search_len_b]
    
    # Find all contiguous matches using a sliding window approach
    best_match = None  # (a_start, b_start, length)
    
    for i in range(len(overlap_a)):
        for j in range(len(overlap_b)):
            if overlap_a[i].id == overlap_b[j].id:
                # Found a potential match start, extend it
                match_len = 1
                while (
                    i + match_len < len(overlap_a)
                    and j + match_len < len(overlap_b)
                    and overlap_a[i + match_len].id == overlap_b[j + match_len].id
                ):
                    match_len += 1
                
                # Update best if this is longer
                if best_match is None or match_len > best_match[2]:
                    best_match = (i, j, match_len)
    
    # Check if we found a good enough match
    if best_match is None or best_match[2] < min_match_len:
        # No good overlap found - just concatenate (this handles non-overlapping chunks)
        return a + b
    
    a_overlap_idx, b_overlap_idx, match_len = best_match
    
    # Convert overlap indices to full array indices
    a_start_idx = len(a) - search_len_a
    
    # Take everything from A up to and INCLUDING the matched region
    # Then take everything from B AFTER the matched region
    cut_a = a_start_idx + a_overlap_idx + match_len  # Include the matched tokens from A
    cut_b = b_overlap_idx + match_len  # Skip the matched tokens in B (they're duplicates)
    
    return a[:cut_a] + b[cut_b:]


def merge_chunks(
    a: list[AlignedToken],
    b: list[AlignedToken],
    *,
    overlap_duration: float,
) -> list[AlignedToken]:
    """Merge two token lists from overlapping chunks."""
    if not a or not b:
        return b if not a else a

    # If tokens don't have per-token timestamps (all same time), use text-based merge
    if _has_uniform_timestamps(a) or _has_uniform_timestamps(b):
        return _merge_text_based(a, b)

    a_end_time = a[-1].end
    b_start_time = b[0].start

    if a_end_time <= b_start_time:
        return a + b

    overlap_a = [token for token in a if token.end > b_start_time - overlap_duration]
    overlap_b = [token for token in b if token.start < a_end_time + overlap_duration]

    enough_pairs = len(overlap_a) // 2

    if len(overlap_a) < 2 or len(overlap_b) < 2:
        # Fall back to text-based merge if overlap detection fails
        return _merge_text_based(a, b)

    best_contiguous = []
    for i in range(len(overlap_a)):
        for j in range(len(overlap_b)):
            if (
                overlap_a[i].id == overlap_b[j].id
                and abs(overlap_a[i].start - overlap_b[j].start) < overlap_duration / 2
            ):
                current = []
                k, l = i, j
                while (
                    k < len(overlap_a)
                    and l < len(overlap_b)
                    and overlap_a[k].id == overlap_b[l].id
                    and abs(overlap_a[k].start - overlap_b[l].start)
                    < overlap_duration / 2
                ):
                    current.append((k, l))
                    k += 1
                    l += 1

                if len(current) > len(best_contiguous):
                    best_contiguous = current

    if len(best_contiguous) >= enough_pairs:
        a_start_idx = len(a) - len(overlap_a)
        lcs_indices_a = [a_start_idx + pair[0] for pair in best_contiguous]
        lcs_indices_b = [pair[1] for pair in best_contiguous]

        result = []
        result.extend(a[: lcs_indices_a[0]])

        for i in range(len(best_contiguous)):
            idx_a = lcs_indices_a[i]
            idx_b = lcs_indices_b[i]

            result.append(a[idx_a])

            if i < len(best_contiguous) - 1:
                next_idx_a = lcs_indices_a[i + 1]
                next_idx_b = lcs_indices_b[i + 1]

                gap_tokens_a = a[idx_a + 1 : next_idx_a]
                gap_tokens_b = b[idx_b + 1 : next_idx_b]

                if len(gap_tokens_b) > len(gap_tokens_a):
                    result.extend(gap_tokens_b)
                else:
                    result.extend(gap_tokens_a)

        result.extend(b[lcs_indices_b[-1] + 1 :])
        return result
    else:
        # Fallback to LCS-based merge
        return _merge_lcs(a, b, overlap_duration=overlap_duration)


def _merge_lcs(
    a: list[AlignedToken],
    b: list[AlignedToken],
    *,
    overlap_duration: float,
) -> list[AlignedToken]:
    """Merge using longest common subsequence."""
    a_end_time = a[-1].end
    b_start_time = b[0].start

    overlap_a = [token for token in a if token.end > b_start_time - overlap_duration]
    overlap_b = [token for token in b if token.start < a_end_time + overlap_duration]

    if len(overlap_a) < 2 or len(overlap_b) < 2:
        cutoff_time = (a_end_time + b_start_time) / 2
        return [t for t in a if t.end <= cutoff_time] + [
            t for t in b if t.start >= cutoff_time
        ]

    dp = [[0 for _ in range(len(overlap_b) + 1)] for _ in range(len(overlap_a) + 1)]

    for i in range(1, len(overlap_a) + 1):
        for j in range(1, len(overlap_b) + 1):
            if (
                overlap_a[i - 1].id == overlap_b[j - 1].id
                and abs(overlap_a[i - 1].start - overlap_b[j - 1].start)
                < overlap_duration / 2
            ):
                dp[i][j] = dp[i - 1][j - 1] + 1
            else:
                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

    lcs_pairs = []
    i, j = len(overlap_a), len(overlap_b)

    while i > 0 and j > 0:
        if (
            overlap_a[i - 1].id == overlap_b[j - 1].id
            and abs(overlap_a[i - 1].start - overlap_b[j - 1].start)
            < overlap_duration / 2
        ):
            lcs_pairs.append((i - 1, j - 1))
            i -= 1
            j -= 1
        elif dp[i - 1][j] > dp[i][j - 1]:
            i -= 1
        else:
            j -= 1

    lcs_pairs.reverse()

    if not lcs_pairs:
        cutoff_time = (a_end_time + b_start_time) / 2
        return [t for t in a if t.end <= cutoff_time] + [
            t for t in b if t.start >= cutoff_time
        ]

    a_start_idx = len(a) - len(overlap_a)
    lcs_indices_a = [a_start_idx + pair[0] for pair in lcs_pairs]
    lcs_indices_b = [pair[1] for pair in lcs_pairs]

    result = []
    result.extend(a[: lcs_indices_a[0]])

    for i in range(len(lcs_pairs)):
        idx_a = lcs_indices_a[i]
        idx_b = lcs_indices_b[i]

        result.append(a[idx_a])

        if i < len(lcs_pairs) - 1:
            next_idx_a = lcs_indices_a[i + 1]
            next_idx_b = lcs_indices_b[i + 1]

            gap_tokens_a = a[idx_a + 1 : next_idx_a]
            gap_tokens_b = b[idx_b + 1 : next_idx_b]

            if len(gap_tokens_b) > len(gap_tokens_a):
                result.extend(gap_tokens_b)
            else:
                result.extend(gap_tokens_a)

    result.extend(b[lcs_indices_b[-1] + 1 :])

    return result

